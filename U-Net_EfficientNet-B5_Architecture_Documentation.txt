================================================================================
                    U-NET MODEL ARCHITECTURE WITH EFFICIENTNET-B5 ENCODER
                           COMPREHENSIVE TECHNICAL DOCUMENTATION
================================================================================

OVERVIEW
========
This document provides a detailed technical specification of the U-Net model 
architecture using EfficientNet-B5 as the encoder backbone for medical image 
segmentation. The model is implemented using PyTorch and segmentation_models_pytorch.

MODEL CONFIGURATION
==================
- Architecture: U-Net++ (UnetPlusPlus)
- Encoder: EfficientNet-B5
- Input Image Size: 640x640 pixels
- Input Channels: 3 (RGB)
- Output Classes: 1 (binary segmentation)
- Total Parameters: 31,910,081
- Trainable Parameters: 31,910,081

PREPROCESSING PIPELINE
=====================

1. INPUT IMAGE SPECIFICATIONS
   - Original Format: Various (JPEG, PNG, etc.)
   - Color Space: RGB (converted from BGR if using OpenCV)
   - Bit Depth: 8-bit (0-255 pixel values)
   - Target Size: 640x640 pixels

2. PREPROCESSING STEPS (IN ORDER)
   Step 1: Image Loading
   - Load image using cv2.imread() or PIL
   - Convert BGR to RGB if using OpenCV: cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
   
   Step 2: Resizing
   - Method: cv2.resize() with cv2.INTER_LINEAR interpolation
   - Target: (640, 640) pixels
   - Preserves aspect ratio by padding if needed
   
   Step 3: Normalization (Albumentations)
   - Mean: [0.485, 0.456, 0.406] (ImageNet statistics)
   - Std: [0.229, 0.224, 0.225] (ImageNet statistics)
   - Max Pixel Value: 255.0 (CRITICAL: Fixed from incorrect 1.0)
   - Formula: (pixel_value / 255.0 - mean) / std
   
   Step 4: Tensor Conversion
   - Convert to PyTorch tensor: ToTensorV2()
   - Shape transformation: (H, W, C) → (C, H, W)
   - Data type: torch.float32
   - Final tensor shape: (3, 640, 640)

EFFICIENTNET-B5 ENCODER ARCHITECTURE
===================================

EFFICIENTNET-B5 SCALING PARAMETERS
- Width Coefficient: 1.6
- Depth Coefficient: 2.2
- Resolution: 456 (base), scaled to 640 for our application
- Dropout Rate: 0.4

ENCODER OUTPUT CHANNELS (Skip Connections)
- Level 0: 3 channels (input RGB)
- Level 1: 48 channels (after stem)
- Level 2: 40 channels (stage 1)
- Level 3: 64 channels (stage 2)
- Level 4: 176 channels (stage 3)
- Level 5: 512 channels (stage 4, bottleneck)

DETAILED LAYER-BY-LAYER ARCHITECTURE
===================================

INPUT LAYER
-----------
Input Shape: (batch_size, 3, 640, 640)
Description: RGB image tensor after preprocessing

STEM BLOCK
----------
Layer: Conv2d
- Input Channels: 3
- Output Channels: 48 (32 * 1.6 width_coefficient)
- Kernel Size: 3x3
- Stride: 2
- Padding: 1 (same padding)
- Bias: False
Output Shape: (batch_size, 48, 320, 320)

BatchNorm2d
- Channels: 48
- Momentum: 0.01 (1 - 0.99)
- Epsilon: 1e-3

Activation: Swish (SiLU)
Output Shape: (batch_size, 48, 320, 320)

MOBILE INVERTED BOTTLENECK BLOCKS (MBConv)
==========================================

STAGE 1: MBConv1 (k3x3_s1_e1_i48_o40_se0.25)
- Repeat: 3 blocks (1 * 2.2 depth_coefficient, rounded)
- Input Channels: 48
- Output Channels: 40
- Kernel Size: 3x3
- Stride: 1
- Expansion Ratio: 1
- SE Ratio: 0.25

Block Structure:
1. Depthwise Conv2d: 48→48, kernel=3x3, stride=1, groups=48
2. BatchNorm2d + Swish
3. Squeeze-and-Excitation: 48→12→48
4. Pointwise Conv2d: 48→40, kernel=1x1
5. BatchNorm2d
6. Skip connection (if input_channels == output_channels)
Output Shape: (batch_size, 40, 320, 320)

STAGE 2: MBConv6 (k3x3_s2_e6_i40_o64_se0.25)
- Repeat: 4 blocks (2 * 2.2 depth_coefficient, rounded)
- Input Channels: 40
- Output Channels: 64
- Kernel Size: 3x3
- Stride: 2 (first block), 1 (subsequent blocks)
- Expansion Ratio: 6
- SE Ratio: 0.25

Block Structure:
1. Pointwise Conv2d: 40→240, kernel=1x1 (expansion)
2. BatchNorm2d + Swish
3. Depthwise Conv2d: 240→240, kernel=3x3, stride=2/1, groups=240
4. BatchNorm2d + Swish
5. Squeeze-and-Excitation: 240→60→240
6. Pointwise Conv2d: 240→64, kernel=1x1
7. BatchNorm2d
8. Skip connection (if applicable)
Output Shape: (batch_size, 64, 160, 160)

STAGE 3: MBConv6 (k5x5_s2_e6_i64_o176_se0.25)
- Repeat: 4 blocks (2 * 2.2 depth_coefficient, rounded)
- Input Channels: 64
- Output Channels: 176
- Kernel Size: 5x5
- Stride: 2 (first block), 1 (subsequent blocks)
- Expansion Ratio: 6
- SE Ratio: 0.25

Block Structure:
1. Pointwise Conv2d: 64→384, kernel=1x1 (expansion)
2. BatchNorm2d + Swish
3. Depthwise Conv2d: 384→384, kernel=5x5, stride=2/1, groups=384
4. BatchNorm2d + Swish
5. Squeeze-and-Excitation: 384→96→384
6. Pointwise Conv2d: 384→176, kernel=1x1
7. BatchNorm2d
8. Skip connection (if applicable)
Output Shape: (batch_size, 176, 80, 80)

STAGE 4: MBConv6 (k3x3_s2_e6_i176_o512_se0.25)
- Repeat: 6 blocks (3 * 2.2 depth_coefficient, rounded)
- Input Channels: 176
- Output Channels: 512
- Kernel Size: 3x3
- Stride: 2 (first block), 1 (subsequent blocks)
- Expansion Ratio: 6
- SE Ratio: 0.25

Block Structure:
1. Pointwise Conv2d: 176→1056, kernel=1x1 (expansion)
2. BatchNorm2d + Swish
3. Depthwise Conv2d: 1056→1056, kernel=3x3, stride=2/1, groups=1056
4. BatchNorm2d + Swish
5. Squeeze-and-Excitation: 1056→264→1056
6. Pointwise Conv2d: 1056→512, kernel=1x1
7. BatchNorm2d
8. Skip connection (if applicable)
Output Shape: (batch_size, 512, 40, 40)

U-NET DECODER ARCHITECTURE
==========================

The decoder uses skip connections from the encoder at multiple scales:
- Skip 1: (batch_size, 48, 320, 320) from stem
- Skip 2: (batch_size, 40, 320, 320) from stage 1
- Skip 3: (batch_size, 64, 160, 160) from stage 2
- Skip 4: (batch_size, 176, 80, 80) from stage 3
- Bottleneck: (batch_size, 512, 40, 40) from stage 4

DECODER CHANNELS: [256, 128, 64, 32, 16]

DECODER BLOCK 1
---------------
Input: (batch_size, 512, 40, 40)
Skip Connection: (batch_size, 176, 80, 80)

1. Upsample: Nearest neighbor interpolation to 80x80
2. Concatenate: 512 + 176 = 688 channels
3. Conv2dReLU: 688→256, kernel=3x3, padding=1
4. BatchNorm2d + ReLU
5. Conv2dReLU: 256→256, kernel=3x3, padding=1
6. BatchNorm2d + ReLU
Output Shape: (batch_size, 256, 80, 80)

DECODER BLOCK 2
---------------
Input: (batch_size, 256, 80, 80)
Skip Connection: (batch_size, 64, 160, 160)

1. Upsample: Nearest neighbor interpolation to 160x160
2. Concatenate: 256 + 64 = 320 channels
3. Conv2dReLU: 320→128, kernel=3x3, padding=1
4. BatchNorm2d + ReLU
5. Conv2dReLU: 128→128, kernel=3x3, padding=1
6. BatchNorm2d + ReLU
Output Shape: (batch_size, 128, 160, 160)

DECODER BLOCK 3
---------------
Input: (batch_size, 128, 160, 160)
Skip Connection: (batch_size, 40, 320, 320)

1. Upsample: Nearest neighbor interpolation to 320x320
2. Concatenate: 128 + 40 = 168 channels
3. Conv2dReLU: 168→64, kernel=3x3, padding=1
4. BatchNorm2d + ReLU
5. Conv2dReLU: 64→64, kernel=3x3, padding=1
6. BatchNorm2d + ReLU
Output Shape: (batch_size, 64, 320, 320)

DECODER BLOCK 4
---------------
Input: (batch_size, 64, 320, 320)
Skip Connection: (batch_size, 48, 320, 320)

1. Upsample: Already at 320x320
2. Concatenate: 64 + 48 = 112 channels
3. Conv2dReLU: 112→32, kernel=3x3, padding=1
4. BatchNorm2d + ReLU
5. Conv2dReLU: 32→32, kernel=3x3, padding=1
6. BatchNorm2d + ReLU
Output Shape: (batch_size, 32, 320, 320)

DECODER BLOCK 5
---------------
Input: (batch_size, 32, 320, 320)
Skip Connection: None

1. Upsample: Nearest neighbor interpolation to 640x640
2. Conv2dReLU: 32→16, kernel=3x3, padding=1
3. BatchNorm2d + ReLU
4. Conv2dReLU: 16→16, kernel=3x3, padding=1
5. BatchNorm2d + ReLU
Output Shape: (batch_size, 16, 640, 640)

SEGMENTATION HEAD
================
Input: (batch_size, 16, 640, 640)

Conv2d:
- Input Channels: 16
- Output Channels: 1
- Kernel Size: 3x3
- Padding: 1
- Bias: True

Output Shape: (batch_size, 1, 640, 640)

ACTIVATION AND OUTPUT
====================
- Sigmoid activation applied during inference
- Output range: [0, 1] (probability map)
- Threshold: 0.5 for binary segmentation
- Final binary mask: (prediction > 0.5).astype(uint8)

TRAINING CONFIGURATION
=====================
- Loss Function: Combined (Dice + BCE)
- Optimizer: Adam
- Learning Rate: 1e-4
- Weight Decay: 1e-5
- Batch Size: 2
- Image Size: 640x640
- Epochs: 50
- Scheduler: ReduceLROnPlateau

DATA AUGMENTATION
================
Training augmentations (applied with albumentations):
- HorizontalFlip (p=0.5)
- VerticalFlip (p=0.2)
- RandomRotate90 (p=0.3)
- ShiftScaleRotate (shift=0.1, scale=0.1, rotate=15°, p=0.5)
- RandomBrightnessContrast (brightness=0.2, contrast=0.2, p=0.5)
- GaussNoise/GaussianBlur/MotionBlur (p=0.3)
- HueSaturationValue/CLAHE/RandomGamma (p=0.3)

INFERENCE WORKFLOW
=================
1. Load image → RGB conversion
2. Resize to 640x640
3. Normalize with ImageNet statistics (max_pixel_value=255.0)
4. Convert to tensor (3, 640, 640)
5. Forward pass through model
6. Apply sigmoid activation
7. Threshold at 0.5 for binary mask
8. Resize back to original image dimensions if needed

PERFORMANCE METRICS
==================
- Best Dice Score: 0.8945
- Training Time: ~0.63 hours (50 epochs)
- Model Size: ~128 MB
- Inference Time: ~50ms per image (GPU)

================================================================================
                                END OF DOCUMENTATION
================================================================================
